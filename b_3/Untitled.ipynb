{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1.0, 2.0, 3.0]\n",
    "y = [2.0, 4.0, 6.0]\n",
    "w = 1.0\n",
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, b=0, w=3):\n",
    "    return x * w + b\n",
    "\n",
    "def loss(x, y, w):\n",
    "    return (x * w - y) ** 2\n",
    "\n",
    "def gradient(x, y, w, alpha):\n",
    "    return w - alpha * 2 * (w * x - y) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 0 w= 1.260688 loss= 4.919240100095999\n",
      "progress: 1 w= 1.453417766656 loss= 2.688769240265834\n",
      "progress: 2 w= 1.5959051959019805 loss= 1.4696334962911515\n",
      "progress: 3 w= 1.701247862192685 loss= 0.8032755585999681\n",
      "progress: 4 w= 1.7791289594933983 loss= 0.43905614881022015\n",
      "progress: 5 w= 1.836707389300983 loss= 0.2399802903801062\n",
      "progress: 6 w= 1.8792758133988885 loss= 0.1311689630744999\n",
      "progress: 7 w= 1.910747160155559 loss= 0.07169462478267678\n",
      "progress: 8 w= 1.9340143044689266 loss= 0.03918700813247573\n",
      "progress: 9 w= 1.9512159834655312 loss= 0.021418922423117836\n",
      "progress: 10 w= 1.9639333911678687 loss= 0.01170720245384975\n",
      "progress: 11 w= 1.9733355232910992 loss= 0.006398948863435593\n",
      "progress: 12 w= 1.9802866323953892 loss= 0.003497551760830656\n",
      "progress: 13 w= 1.9854256707695 loss= 0.001911699652671057\n",
      "progress: 14 w= 1.9892250235079405 loss= 0.0010449010656399273\n",
      "progress: 15 w= 1.9920339305797026 loss= 0.0005711243580809696\n",
      "progress: 16 w= 1.994110589284741 loss= 0.0003121664271570621\n",
      "progress: 17 w= 1.9956458879852805 loss= 0.0001706246229305199\n",
      "progress: 18 w= 1.9967809527381737 loss= 9.326038746484765e-05\n",
      "progress: 19 w= 1.9976201197307648 loss= 5.097447086306101e-05\n",
      "progress: 20 w= 1.998240525958391 loss= 2.7861740127856012e-05\n",
      "progress: 21 w= 1.99869919972735 loss= 1.5228732143933469e-05\n",
      "progress: 22 w= 1.9990383027488265 loss= 8.323754426231206e-06\n",
      "progress: 23 w= 1.9992890056818404 loss= 4.549616284094891e-06\n",
      "progress: 24 w= 1.999474353368653 loss= 2.486739429417538e-06\n",
      "progress: 25 w= 1.9996113831376856 loss= 1.3592075910762856e-06\n",
      "progress: 26 w= 1.9997126908902887 loss= 7.429187207079447e-07\n",
      "progress: 27 w= 1.9997875889274812 loss= 4.060661735575354e-07\n",
      "progress: 28 w= 1.9998429619451539 loss= 2.2194855602869353e-07\n",
      "progress: 29 w= 1.9998838998815958 loss= 1.213131374411496e-07\n",
      "progress: 30 w= 1.9999141657892625 loss= 6.630760559646474e-08\n",
      "progress: 31 w= 1.9999365417379913 loss= 3.624255915449335e-08\n",
      "progress: 32 w= 1.9999530845453979 loss= 1.9809538924707548e-08\n",
      "progress: 33 w= 1.9999653148414271 loss= 1.0827542027017377e-08\n",
      "progress: 34 w= 1.999974356846045 loss= 5.9181421028034105e-09\n",
      "progress: 35 w= 1.9999810417085633 loss= 3.2347513278475087e-09\n",
      "progress: 36 w= 1.9999859839076413 loss= 1.7680576050779005e-09\n",
      "progress: 37 w= 1.9999896377347262 loss= 9.6638887447731e-10\n",
      "progress: 38 w= 1.999992339052936 loss= 5.282109892545845e-10\n",
      "progress: 39 w= 1.9999943361699042 loss= 2.887107421958329e-10\n",
      "progress: 40 w= 1.9999958126624442 loss= 1.5780416225633037e-10\n",
      "progress: 41 w= 1.999996904251097 loss= 8.625295142578772e-11\n",
      "progress: 42 w= 1.999997711275687 loss= 4.71443308235547e-11\n",
      "progress: 43 w= 1.9999983079186507 loss= 2.5768253628059826e-11\n",
      "progress: 44 w= 1.9999987490239537 loss= 1.4084469615916932e-11\n",
      "progress: 45 w= 1.9999990751383971 loss= 7.698320862431846e-12\n",
      "progress: 46 w= 1.9999993162387186 loss= 4.20776540913866e-12\n",
      "progress: 47 w= 1.9999994944870796 loss= 2.299889814334344e-12\n",
      "progress: 48 w= 1.9999996262682318 loss= 1.2570789110540446e-12\n",
      "progress: 49 w= 1.999999723695619 loss= 6.870969979249939e-13\n",
      "progress: 50 w= 1.9999997957248556 loss= 3.7555501141274804e-13\n",
      "progress: 51 w= 1.9999998489769344 loss= 2.052716967104274e-13\n",
      "progress: 52 w= 1.9999998883468353 loss= 1.1219786256679713e-13\n",
      "progress: 53 w= 1.9999999174534755 loss= 6.132535848018759e-14\n",
      "progress: 54 w= 1.999999938972364 loss= 3.351935118167793e-14\n",
      "progress: 55 w= 1.9999999548815364 loss= 1.8321081844499955e-14\n",
      "progress: 56 w= 1.9999999666433785 loss= 1.0013977760018664e-14\n",
      "progress: 57 w= 1.9999999753390494 loss= 5.473462367088053e-15\n",
      "progress: 58 w= 1.9999999817678633 loss= 2.991697274308627e-15\n",
      "progress: 59 w= 1.9999999865207625 loss= 1.6352086111474931e-15\n",
      "progress: 60 w= 1.999999990034638 loss= 8.937759877335403e-16\n",
      "progress: 61 w= 1.9999999926324883 loss= 4.885220495987371e-16\n",
      "progress: 62 w= 1.99999999455311 loss= 2.670175009618106e-16\n",
      "progress: 63 w= 1.9999999959730488 loss= 1.4594702493172377e-16\n",
      "progress: 64 w= 1.9999999970228268 loss= 7.977204100704301e-17\n",
      "progress: 65 w= 1.9999999977989402 loss= 4.360197735196887e-17\n",
      "progress: 66 w= 1.9999999983727301 loss= 2.3832065197304227e-17\n",
      "progress: 67 w= 1.9999999987969397 loss= 1.3026183953845832e-17\n",
      "progress: 68 w= 1.999999999110563 loss= 7.11988308874388e-18\n",
      "progress: 69 w= 1.9999999993424284 loss= 3.89160224698574e-18\n",
      "progress: 70 w= 1.9999999995138495 loss= 2.1270797208746147e-18\n",
      "progress: 71 w= 1.9999999996405833 loss= 1.1626238773828175e-18\n",
      "progress: 72 w= 1.999999999734279 loss= 6.354692062078993e-19\n",
      "progress: 73 w= 1.9999999998035491 loss= 3.4733644793346653e-19\n",
      "progress: 74 w= 1.9999999998547615 loss= 1.8984796531526204e-19\n",
      "progress: 75 w= 1.9999999998926234 loss= 1.0376765851119951e-19\n",
      "progress: 76 w= 1.9999999999206153 loss= 5.671751114309842e-20\n",
      "progress: 77 w= 1.9999999999413098 loss= 3.100089617511693e-20\n",
      "progress: 78 w= 1.9999999999566096 loss= 1.6944600977692705e-20\n",
      "progress: 79 w= 1.9999999999679208 loss= 9.2616919156479e-21\n",
      "progress: 80 w= 1.9999999999762834 loss= 5.062350511130293e-21\n",
      "progress: 81 w= 1.999999999982466 loss= 2.7669155644059242e-21\n",
      "progress: 82 w= 1.9999999999870368 loss= 1.5124150106147723e-21\n",
      "progress: 83 w= 1.999999999990416 loss= 8.26683933105326e-22\n",
      "progress: 84 w= 1.9999999999929146 loss= 4.518126871054872e-22\n",
      "progress: 85 w= 1.9999999999947617 loss= 2.469467919185614e-22\n",
      "progress: 86 w= 1.9999999999961273 loss= 1.349840097651456e-22\n",
      "progress: 87 w= 1.999999999997137 loss= 7.376551550022107e-23\n",
      "progress: 88 w= 1.9999999999978835 loss= 4.031726170507742e-23\n",
      "progress: 89 w= 1.9999999999984353 loss= 2.2033851437431755e-23\n",
      "progress: 90 w= 1.9999999999988431 loss= 1.2047849775995315e-23\n",
      "progress: 91 w= 1.9999999999991447 loss= 6.5840863393251405e-24\n",
      "progress: 92 w= 1.9999999999993676 loss= 3.5991747246272455e-24\n",
      "progress: 93 w= 1.9999999999995324 loss= 1.969312363793734e-24\n",
      "progress: 94 w= 1.9999999999996543 loss= 1.0761829795642296e-24\n",
      "progress: 95 w= 1.9999999999997444 loss= 5.875191475205477e-25\n",
      "progress: 96 w= 1.999999999999811 loss= 3.2110109830478153e-25\n",
      "progress: 97 w= 1.9999999999998603 loss= 1.757455879087579e-25\n",
      "progress: 98 w= 1.9999999999998967 loss= 9.608404711682446e-26\n",
      "progress: 99 w= 1.9999999999999236 loss= 5.250973729513143e-26\n",
      "predict (after training) 4 hours 7.9999999999996945\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    for i in range(len(x)):\n",
    "        w = gradient(x[i], y[i], w, alpha)\n",
    "        l = loss(x[i], y[i], w)\n",
    "    print(\"progress:\", epoch, \"w=\", w, \"loss=\", l)\n",
    "print(\"predict (after training)\",  \"4 hours\", forward(4, w=w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 4.0\n"
     ]
    }
   ],
   "source": [
    "print(\"predict (before training)\",  4, forward(4, w=w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
